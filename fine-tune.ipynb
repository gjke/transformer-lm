{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path('/Users/gjke/Documents/uni/faktual/s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUKET_NAME = 'gpt2-finetuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_directory_from_s3(bucket_name, remote_directory_name, local_destination ):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name) \n",
    "    for object in bucket.objects.filter(Prefix = remote_directory_name):\n",
    "        print(object, os.path.dirname(object.key))\n",
    "        path, filename = os.path.split(object.key)\n",
    "        if filename == '':\n",
    "            continue\n",
    "        # boto3 s3 download_file will throw exception if folder not exists\n",
    "        try:\n",
    "            os.makedirs(os.path.join(local_destination,path)) \n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        bucket.download_file(object.key, os.path.join(local_destination, object.key))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_directory_from_s3(BUKET_NAME, 'swissdata', PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_directory_from_s3(BUKET_NAME, 'de345-root', PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add <tldr> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm.inference import ModelWrapper\n",
    "import sentencepiece_model_pb2 as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SP_MODEL = os.path.join(PATH, 'de345-root' 'sp.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.ModelProto()\n",
    "m.ParseFromString(open(PATH_TO_SP_MODEL, 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_piece = m.pieces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "new_piece = copy(first_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_piece.Clear()\n",
    "new_piece.piece = '<tldr>'\n",
    "new_piece.score = 0.0\n",
    "new_piece.type = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.pieces.pop()\n",
    "m.pieces.append(new_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m.pieces), m.pieces[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TO_SP_MODEL , 'wb') as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = os.path.join(PATH, 'swissdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = os.path.join(PATH, 'de345-root')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat train.csv using <tldr\\> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH_TO_DATA / 'data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TLDR = '<tldr>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PATH_TO_DATA / 'train'):\n",
    "    os.mkdir(PATH_TO_DATA / 'train')\n",
    "with open(PATH_TO_DATA / 'train/data_train_tldr.txt', 'w') as f:\n",
    "    for row in train_df.loc[:int(train_df.shape[0]*0.85)].itertuples(index=False):\n",
    "        f.write(row[0] + TLDR + row[1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PATH_TO_DATA / 'valid'):\n",
    "    os.mkdir(PATH_TO_DATA / 'valid')\n",
    "with open(PATH_TO_DATA / 'valid/data_val_tldr.txt', 'w') as f:\n",
    "    for row in train_df.loc[int(train_df.shape[0]*0.85):int(train_df.shape[0]*0.95)].itertuples(index=False):\n",
    "        f.write(row[0] + TLDR + row[1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PATH_TO_DATA / 'test'):\n",
    "    os.mkdir(PATH_TO_DATA / 'test')\n",
    "with open(PATH_TO_DATA / 'test/data_test_tldr.txt', 'w') as f:\n",
    "    for row in train_df.loc[int(train_df.shape[0]*0.95):].itertuples(index=False):\n",
    "        f.write(row[0] + TLDR + row[1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sp-encode $PATH_TO_DATA $PATH_TO_MODEL/sp.model $PATH_TO_DATA/encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpt-2 $PATH_TO_MODEL $PATH_TO_DATA/encoded $PATH_TO_MODEL/sp.model --n_embed=1024 --n_ctx=1024 --n_head=16 --n_layer=24 --n_hidden=1024 --batch_size=3 --gradient_checkpointing=0 --save_every=5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('german-gpt2': conda)",
   "language": "python",
   "name": "python361064bitgermangpt2condad2833cb9c60e43939b238a6700187608"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
